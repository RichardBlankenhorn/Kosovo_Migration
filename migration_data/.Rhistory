library(gbm)
set.seed(702)
for (i in lambdas){
tree.boost <- gbm(Salary~.,data=hit.train, distribution='gaussian',n.trees=1000,shrinkage=i)
pred <- predict(tree.boost,newdata=hit.train,n.trees=1000)
mse.vals[[i]] <- mean((pred - hit.train$Salary)^2)
}
new.hit$Salary <- log(new.hit$Salary)
hit.train <- new.hit[1:200,]
hit.test <- new.hit[201:263,]
set.seed(702)
for (i in lambdas){
tree.boost <- gbm(Salary~.,data=hit.train, distribution='gaussian',n.trees=1000,shrinkage=i)
pred <- predict(tree.boost,newdata=hit.train,n.trees=1000)
mse.vals[[i]] <- mean((pred - hit.train$Salary)^2)
}
set.seed(702)
for (i in lambdas){
tree.boost <- gbm(Salary~.,data=hit.train, distribution='gaussian',n.trees=1000,shrinkage=i)
pred <- predict(tree.boost,hit.train,n.trees=1000)
mse.vals[i] <- mean((pred - hit.train$Salary)^2)
}
plot(mse.vals, lambdas)
mse.vals
lambdas <- seq(from=0.00005,to=0.5,.005)
lambdas
lambdas <- seq(from=0.00005,to=0.5,.0005)
lambdas
lambdas <- seq(from=0.00005,to=0.05,.0005)
lambdas
preds <- rep(0,length(lambdas))
mse.vals <- rep(0, length(lambdas))
set.seed(702)
for (i in lambdas){
tree.boost <- gbm(Salary~.,data=hit.train, distribution='gaussian',n.trees=1000,shrinkage=i)
pred <- predict(tree.boost,hit.train,n.trees=1000)
mse.vals[i] <- mean((pred - hit.train$Salary)^2)
}
plot(mse.vals, lambdas)
mse.vals
lambdas <- seq(from=0.00005,to=0.05,.0005)
lambdas
preds <- rep(0,length(lambdas))
mse.vals <- rep(0, length(lambdas))
set.seed(702)
for (i in lambdas){
tree.boost <- gbm(Salary~.,data=hit.train, distribution='gaussian',n.trees=1000,shrinkage=i,verbose = F)
pred <- predict(tree.boost,hit.train,n.trees=1000)
mse.vals[i] <- mean((pred - hit.train$Salary)^2)
}
plot(mse.vals, lambdas)
mse.vals
pows <- seq(-10, -0.2, by = 0.1)
lambdas <- 10^pows
lambdas
round(lambdas, 10)
treeb <- gbm(Salary ~ ., data = hit.train, distribution = 'gaussian', n.trees = 1000,
shrinkage = 0.1, verbose = F)
pred <- predict(treeb, n.trees = 1000)
mse <- mean((pred - hit.train$Salary)^2)
mse
lambdas <- seq(from=0.00005,to=0.05,.0005)
for (i in lambdas){print i}
for (i in lambdas){print(i)}
lambdas <- seq(from=0.00005,to=0.05,.0005)
preds <- rep(0,length(lambdas))
mse.vals <- rep(0, length(lambdas))
set.seed(702)
for (i in lambdas){
tree.boost <- gbm(Salary~.,data=hit.train, distribution='gaussian',n.trees=1000,shrinkage=i,verbose = F)
pred <- predict(tree.boost,hit.train,n.trees=1000)
mse.val <- mean((pred - hit.train$Salary)^2)
print(mse.val)
}
mse.vals
lambdas <- seq(from=0.00005,to=0.05,.0005)
preds <- rep(0,length(lambdas))
mse.vals <- rep(0, length(lambdas))
set.seed(702)
for (i in lambdas && k in length(lambdas)){
tree.boost <- gbm(Salary~.,data=hit.train, distribution='gaussian',n.trees=1000,shrinkage=i,verbose = F)
pred <- predict(tree.boost,hit.train,n.trees=1000)
mse.vals[[k]] <- mean((pred - hit.train$Salary)^2)
}
lambdas <- seq(from=0.00005,to=0.05,.0005)
preds <- rep(0,length(lambdas))
mse.vals <- rep(0, length(lambdas))
set.seed(702)
ix <- 1
for (i in lambdas){
tree.boost <- gbm(Salary~.,data=hit.train, distribution='gaussian',n.trees=1000,shrinkage=i,verbose = F)
pred <- predict(tree.boost,hit.train,n.trees=1000)
mse.vals[[ix]] <- mean((pred - hit.train$Salary)^2)
ix <- ix + 1
}
plot(mse.vals, lambdas)
head(Hitters)
plot(x,y)
x <- c(-1,0,1,2)
y <- c(-1,0,1,2)
plot(x,y)
plot(x,y,bg='white')
plot(x,y,bg='white',col='white')
abline(2,0)
x <- c(-1,0,1,2,3)
y <- c(-1,0,1,2,3)
plot(x,y,bg='white',col='white')
abline(2,0)
abline(1,0)
segments(0,1,0,2)
segments(1,0,1,1)
x <- c(-1,0,1,2,3)
y <- c(0,1,2,3)
plot(x,y,bg='white',col='white')
x <- c(-1,0,1,2,3)
y <- c(0,1,2,3,4)
plot(x,y,bg='white',col='white')
abline(2,0)
abline(1,0)
segments(0,1,0,2)
segments(1,0,1,1)
x <- c(-1,0,1,2,3)
y <- c(-1,0,1,2,3)
plot(x,y,bg='white',col='white')
abline(2,0)
abline(1,0)
segments(0,1,0,2)
segments(1,0,1,1)
segments(1,-1,1,1)
text(0,0,"-1.80")
text(-.5,1.5,"-1.06")
text(2,0,".63")
text(1.5,1.5,".21")
library(e1071)
set.seed(1)
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1, 10), rep(1,10))
x[y==1,] <- x[y==1,] + 1
plot(x, col = (3-y))
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = 'linear', cost = 10, scale = F)
plot(svmfit, dat)
par(mfrow=c(1,2))
plot(x, col = (3-y))
plot(svmfit, dat)
par(mfrow=c(1,2))
plot(x, col = (3-y))
plot(svmfit, dat)
svmfit$index
summary(svmfit)
svmfit2 <- svm(y ~ . data = dat, kernel = 'linear', cost = 0.1, scale = F)
svmfit2 <- svm(y ~ ., data = dat, kernel = 'linear', cost = 0.1, scale = F)
plot(svmfit2, dat)
svmfit2$index
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat, kernel = 'linear',
ranges = list(cost=c(.001,.01,.1,1,5,10,100)))
summary(tune.out)
bestmod <- tune.out$best.model
summary(bestmod)
xtest <- matrix(rnorm(20*2), ncol = 2)
ytest <- sample(c(-1,1), 20, rep=T)
xtest[ytest==1,] <- xtest[ytest==1,] + 1
testdata <- data.frame(x=xtest,y=as.factor(ytest))
pred <- predict(bestmod, testdata)
table(predict=pred, truth=testdata$y)
x[y==1,] <- x[y==1,] + .5
plot(x, col = (y+5)/2, pch = 19)
dat <- data.frame(x=x, y=as.factor(y))
svmfit3 <- svm(y ~ ., data = dat, kernel = 'linear', cost = 1e5)
summary(svmfit3)
plot(svmfit3, dat)
svmfit4 <- svm(y ~ ., data = dat, kernel = 'linear', cost = 1)
summary(svmfit4)
plot(svmfit4, dat)
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] <- x[1:100,] + 2
x[101:150,] <- x[101:150,] - 2
y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x=x,y=as.factor(y))
head(dat)
plot(x, col = y)
train <- sample(200,100)
svmfit5 <- svm(y ~ ., data = dat[train,], kernel = 'radial', gamma = 1, cost =  1)
plot(svmfit, dat[train,])
summary(svmfit5)
set.seed(1)
train <- sample(200,100)
svmfit5 <- svm(y ~ ., data = dat[train,], kernel = 'radial', gamma = 1, cost =  1)
plot(svmfit, dat[train,])
summary(svmfit5)
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat[train,], kernel = 'radial',
ranges = list(cost=c(0.1,1,10,100,1000), gamma = c(.5,1,2,3,4)))
summary(tune.out)
table(true=dat[-train,"y"], pred=predict(tune.out$best.model, newdata = dat[-train,]))
s <- summary(tune.out) # shows best cost = 1 and gamma = 1
s$best.parameters
s$performances
s$best.model
s
s$performances
data(Auto)
Auto$med.mpg <- ifelse(Auto$mpg < median(Auto$mpg), 0, 1)
Auto$med.mpg <- as.factor(Auto$med.mpg)
Auto$name <- NULL
Auto$mpg <- NULL
set.seed(702)
svm.tune2 <- tune(svm,med.mpg~.,data=Auto,kernel='radial',ranges=list(cost=c(.01,.05,.1,1,10,100,1000)), gamma=c(.01,.05,.1,1,2,5,10))
summary(svm.tune2)
set.seed(1)
tune.out <- tune(svm, med.mpg ~ ., data = Auto, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
summary(tune.out)
summary(svm.tune2)
summary(tune.out)
tune.out2 <- tune(svm, med.mpg ~ ., data = Auto, kernel = 'polynomial', ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2,3,4,5))
tune.out2 <- tune(svm, med.mpg ~ ., data = Auto, kernel = 'polynomial', ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2,3,4,5)))
tune.out2 <- tune(svm, med.mpg ~ ., data = Auto, kernel = 'polynomial', ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2,3,4,5)))
summary(tune.out2)
?pander
library(pander)
?pander
data("OJ")
head(OJ, n = 50)
?svm
svm.fit <- svm(Purchase ~ ., data = OJ, kernel = 'linear', cost = 0.01)
summary(svm.fit)
svm.sum <- summary(svm.fit)
svm.sum$nclasses
svm.sum$decision.values
svm.sum$SV
svm.sum$decision.values
svm.sum$gamma
svm.sum$coef0
svm.sum$nu
svm.sum$epsilon
svm.sum$nclasses
svm.sum$levels
svm.sum$x.scale
svm.sum$y.scale
svm.sum$index
svm.sum$tot.nSV
svm.sum$nSV
k <- 'hello'
paste('Are you going to say', k)
paste('Are you going to say', k, 'because it is rude if you do not')
svm.sum$fitted
data("OJ")
set.seed(702)
n <- nrow(OJ)
samp <- sample(n, 800, replace = F)
train <- OJ[samp,]
test <- OJ[-samp,]
head(OJ)
svm.mod <- svm(Purchase ~ ., data = train, kernel = 'linear', cost = 0.01)
svm.sum <- summary(svm.mod)
svm.sum
train.pred <- predict(svm.mod, newdata = train)
train.table <- table(train.pred, train$Purchase)
train.table
test.table
test.pred <- predict(svm.mod, test)
test.table <- table(test.pred, test$Purchase)
test.table
set.seed(1)
n <- nrow(OJ)
samp <- sample(n, 800, replace = F)
train <- OJ[samp,]
test <- OJ[-samp,]
# Fit SVM Mod and Produce Summary
svm.mod <- svm(Purchase ~ ., data = train, kernel = 'linear', cost = 0.01)
svm.sum <- summary(svm.mod)
svm.sum
# Training Predictions & Table
train.pred <- predict(svm.mod, newdata = train)
train.table <- table(train.pred, train$Purchase)
train.table
set.seed(1)
n <- nrow(OJ)
samp <- sample(n, 800, replace = F)
oj.train <- OJ[samp,]
oj.test <- OJ[-samp,]
# Fit SVM Mod and Produce Summary
svm.mod <- svm(Purchase ~ ., data = oj.train, kernel = 'linear', cost = 0.01)
svm.sum <- summary(svm.mod)
svm.sum
# Training Predictions & Table
train.pred <- predict(svm.mod, newdata = oj.train)
train.table <- table(train.pred, oj.train$Purchase)
train.table
train.error <- 1 - (sum(diag(train.table)) / sum(train.table))
test.error <- 1 - (sum(diag(test.table)) / sum(test.table))
train.error
test.error
set.seed(1)
n <- nrow(OJ)
samp <- sample(n, 800, replace = F)
train <- OJ[samp,]
test <- OJ[-samp,]
# Fit SVM Mod and Produce Summary
svm.mod <- svm(Purchase ~ ., data = train, kernel = 'linear', cost = 0.01)
svm.sum <- summary(svm.mod)
svm.sum
# Training Predictions & Table
train.pred <- predict(svm.mod, newdata = train)
train.table <- table(train.pred, train$Purchase)
train.table
# Test Predictions & Table
test.pred <- predict(svm.mod, test)
test.table <- table(test.pred, test$Purchase)
test.table
# Errors
train.error <- 1 - (sum(diag(train.table)) / sum(train.table))
test.error <- 1 - (sum(diag(test.table)) / sum(test.table))
train.error
test.error
set.seed(3)
n <- nrow(OJ)
samp <- sample(n, 800, replace = F)
train <- OJ[samp,]
test <- OJ[-samp,]
# Fit SVM Mod and Produce Summary
svm.mod <- svm(Purchase ~ ., data = train, kernel = 'linear', cost = 0.01)
svm.sum <- summary(svm.mod)
svm.sum
# Training Predictions & Table
train.pred <- predict(svm.mod, newdata = train)
train.table <- table(train.pred, train$Purchase)
train.table
# Test Predictions & Table
test.pred <- predict(svm.mod, test)
test.table <- table(test.pred, test$Purchase)
test.table
# Errors
train.error <- 1 - (sum(diag(train.table)) / sum(train.table))
test.error <- 1 - (sum(diag(test.table)) / sum(test.table))
train.error
test.error
set.seed(5)
n <- nrow(OJ)
samp <- sample(n, 800, replace = F)
train <- OJ[samp,]
test <- OJ[-samp,]
# SAMPLE II
set.seed(702)
samp2 <- sample(n, 800, replace = F)
train2 <- OJ[samp2,]
test2 <- OJ[-samp2,]
# Fit SVM Mod and Produce Summary
svm.mod <- svm(Purchase ~ ., data = train, kernel = 'linear', cost = 0.01)
svm.mod2 <- svm(Purchase ~ ., data = train2, kernel = 'linear', cost = 0.01)
svm.sum <- summary(svm.mod)
svm.sum2 <- summary(svm.mod2)
svm.sum
svm.sum2
# Training Predictions & Table
train.pred <- predict(svm.mod, train)
train.pred2 <- predict(svm.mod2, train2)
train.table <- table(train.pred, train$Purchase)
train.table2 <- table(train.pred2, train2$Purchase)
train.table
train.table2
# Test Predictions & Table
test.pred <- predict(svm.mod, test)
test.pred2 <- predict(svm.mod2, test2)
test.table <- table(test.pred, test$Purchase)
test.table2 <- table(test.pred2, test2$Purchase)
test.table
test.table2
# Errors
train.error <- 1 - (sum(diag(train.table)) / sum(train.table))
test.error <- 1 - (sum(diag(test.table)) / sum(test.table))
train.error2 <- 1 - (sum(diag(train.table2)) / sum(train.table2))
test.error2 <- 1 - (sum(diag(test.table2)) / sum(test.table2))
train.error
test.error
train.error2
test.error2
set.seed(53)
n <- nrow(OJ)
samp <- sample(n, 800, replace = F)
train <- OJ[samp,]
test <- OJ[-samp,]
# SAMPLE II
set.seed(702)
samp2 <- sample(n, 800, replace = F)
train2 <- OJ[samp2,]
test2 <- OJ[-samp2,]
# Fit SVM Mod and Produce Summary
svm.mod <- svm(Purchase ~ ., data = train, kernel = 'linear', cost = 0.01)
svm.mod2 <- svm(Purchase ~ ., data = train2, kernel = 'linear', cost = 0.01)
svm.sum <- summary(svm.mod)
svm.sum2 <- summary(svm.mod2)
svm.sum
svm.sum2
# Training Predictions & Table
train.pred <- predict(svm.mod, train)
train.pred2 <- predict(svm.mod2, train2)
train.table <- table(train.pred, train$Purchase)
train.table2 <- table(train.pred2, train2$Purchase)
train.table
train.table2
# Test Predictions & Table
test.pred <- predict(svm.mod, test)
test.pred2 <- predict(svm.mod2, test2)
test.table <- table(test.pred, test$Purchase)
test.table2 <- table(test.pred2, test2$Purchase)
test.table
test.table2
# Errors
train.error <- 1 - (sum(diag(train.table)) / sum(train.table))
test.error <- 1 - (sum(diag(test.table)) / sum(test.table))
train.error2 <- 1 - (sum(diag(train.table2)) / sum(train.table2))
test.error2 <- 1 - (sum(diag(test.table2)) / sum(test.table2))
train.error
test.error
train.error2
test.error2
set.seed(3)
n <- nrow(OJ)
samp <- sample(n, 800, replace = F)
train <- OJ[samp,]
test <- OJ[-samp,]
# SAMPLE II
set.seed(702)
samp2 <- sample(n, 800, replace = F)
train2 <- OJ[samp2,]
test2 <- OJ[-samp2,]
# Fit SVM Mod and Produce Summary
svm.mod <- svm(Purchase ~ ., data = train, kernel = 'linear', cost = 0.01)
svm.mod2 <- svm(Purchase ~ ., data = train2, kernel = 'linear', cost = 0.01)
svm.sum <- summary(svm.mod)
svm.sum2 <- summary(svm.mod2)
svm.sum
svm.sum2
# Training Predictions & Table
train.pred <- predict(svm.mod, train)
train.pred2 <- predict(svm.mod2, train2)
train.table <- table(train.pred, train$Purchase)
train.table2 <- table(train.pred2, train2$Purchase)
train.table
train.table2
# Test Predictions & Table
test.pred <- predict(svm.mod, test)
test.pred2 <- predict(svm.mod2, test2)
test.table <- table(test.pred, test$Purchase)
test.table2 <- table(test.pred2, test2$Purchase)
test.table
test.table2
# Errors
train.error <- 1 - (sum(diag(train.table)) / sum(train.table))
test.error <- 1 - (sum(diag(test.table)) / sum(test.table))
train.error2 <- 1 - (sum(diag(train.table2)) / sum(train.table2))
test.error2 <- 1 - (sum(diag(test.table2)) / sum(test.table2))
train.error
test.error
train.error2
test.error2
op <- par(mfrow=c(2,2))
curve(x^3-3*x, -2, 2)
curve(x^2-2, add = TRUE, col = "violet")
curve(x^3-3*x, -2, 2)
curve(x^2-2, add = TRUE, col = "violet")
curve((1+x)^2 + (2-y)^2, -4, 4)
curve((1+x)^2 + (2-y)^2 = 4, -4, 4)
curve((1+x)^2 + (2-y)^2 - 4, -4, 4)
curve((1+x)^2 + (2-x)^2 - 4, -4, 4)
curve((1+x)^2 + (2-x)^2 - 4, -4, 4)
curve((1+x)^2 + (2-y)^2 - 4, c(-4,4), c(-4,4))
curve((1+x)^2 + (2-y)^2 - 4, c(-4,4), c(-4,4))
install.packages(ROCR)
library(ROCR)
library("ROCR")
library("gplots")
install.packages("gplots")
library(pROC)
install.packages("ROCR")
?draw.circle
library(plotrix)
?draw.circle
draw.circle(-1, 2, radius = 2, nv = 100, col = 'blue')
draw.circle(-1, 2, radius = 2, nv = 100)
plot(1:5,seq(1,10,length=5),type="n",xlab="",ylab="",main="Test draw.circle")
plot(1:5,seq(1,10,length=5),type="n",xlab="",ylab="",main="Test draw.circle")
draw.circle(-1, 2, radius = 2, nv = 100)
plot(-2:5,seq(-2,6,length=5),type="n",xlab="",ylab="",main="Test draw.circle")
plot(-2:6,seq(-2,6,length=5),type="n",xlab="",ylab="",main="Test draw.circle")
plot(-2:6,seq(-2,6),type="n",xlab="",ylab="",main="Test draw.circle")
draw.circle(-1, 2, radius = 2, nv = 100)
draw.circle(-1, 2, radius = 2, nv = 10)
draw.circle(-1, 2, radius = 2, nv = 1000)
?draw.circle
plot(-2:5,seq(-2,5),asp = 1, type="n",xlab="",ylab="",main="Test draw.circle")
draw.circle(-1, 2, radius = 2, nv = 1000)
plot(-2:5,-2:5, asp = 1, main = 'Test Draw Circle')
plot(-2:5,-2:5, asp = 1, type = 'n', main = 'Test Draw Circle')
plot(-2:5,-2:3, asp = 1, type = 'n', main = 'Test Draw Circle')
draw.circle(-1, 2, radius = 2, nv = 1000)
draw.circle(1, -2, radius = 2, nv = 1000)
draw.circle(1, 2, radius = 2, nv = 1000)
draw.circle(-1, 2, radius = 2, nv = 1000)
plot(-2:5,-2:5, asp = 1, type = 'n', main = 'Test Draw Circle')
draw.circle(-1, 2, radius = 2, nv = 1000)
text(-1,2,'test')
text(-1,2,'')
text(-1,2,'hello')
setwd("/Users/richardblankenhorn/workspace/Applied_StatsII/Final_Project/Kosovo_Migration/migration_data")
Kos_MBorder <- read.csv('kosovo-morina-border.csv')
head(Kos_MBorder)
summary(Kos_MBorder)
dim(Kos_MBorder)
